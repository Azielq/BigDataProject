<div align="center">

# ğŸš€ AWS Big Data Architecture

### ImplementaciÃ³n de Arquitectura Big Data en AWS con Servicios Serverless

[![AWS](https://img.shields.io/badge/AWS-Cloud-FF9900?style=for-the-badge&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)
[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Completed-success?style=for-the-badge)]()

*SoluciÃ³n cloud completa para procesamiento de datos masivos utilizando servicios nativos de AWS*

[Arquitectura](#-arquitectura) â€¢
[Quick Start](#-quick-start) â€¢
[DocumentaciÃ³n](#-documentaciÃ³n) â€¢
[Consultas SQL](#-consultas-de-ejemplo) â€¢
[VisualizaciÃ³n](#-amazon-quicksight)

---

</div>

## ğŸ“‹ Tabla de Contenidos

- [IntroducciÃ³n](#-introducciÃ³n)
- [Objetivos](#-objetivos)
- [Dataset](#-dataset)
- [Arquitectura](#-arquitectura)
- [Servicios AWS](#-servicios-aws-utilizados)
- [Requisitos Previos](#-requisitos-previos)
- [Quick Start](#-quick-start)
- [ConfiguraciÃ³n Detallada](#-configuraciÃ³n-detallada)
- [Consultas SQL](#-consultas-de-ejemplo)
- [VisualizaciÃ³n](#-amazon-quicksight)
- [Costos](#-costos-y-free-tier)
- [Seguridad](#-seguridad-y-privacidad)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Roadmap](#-roadmap)
- [Autor](#-autor)

---

## ğŸ¯ IntroducciÃ³n

Este proyecto implementa una **arquitectura serverless en la nube** orientada al procesamiento de datos masivos utilizando servicios nativos de AWS. La soluciÃ³n permite:

| Capacidad | DescripciÃ³n |
|-----------|-------------|
| ğŸ“¦ **Almacenamiento** | Dataset real almacenado en S3 con alta durabilidad |
| ğŸ”„ **TransformaciÃ³n** | Limpieza y normalizaciÃ³n automÃ¡tica de datos |
| ğŸ” **Consultas** | AnÃ¡lisis SQL sin administrar servidores |
| ğŸ“Š **VisualizaciÃ³n** | Dashboards interactivos a nivel de negocio |

> [!NOTE]
> La soluciÃ³n aprovecha el **Free Tier de AWS**, optimizando costos para entornos acadÃ©micos y de desarrollo.

---

## ğŸ¯ Objetivos

### Objetivo General

Implementar una soluciÃ³n cloud bajo el paradigma Big Data utilizando AWS, que permita cargar un dataset real, limpiarlo mediante backend serverless, generar consultas analÃ­ticas con SQL y visualizar resultados empresariales.

### Objetivos EspecÃ­ficos

- [x] Comprender el ecosistema de servicios Big Data en AWS
- [x] DiseÃ±ar arquitectura con almacenamiento, procesamiento, consultas y visualizaciÃ³n
- [x] Implementar flujo de trabajo sobre dataset de taxis NYC
- [x] Evaluar aspectos de costos y limitaciones del Free Tier
- [x] Documentar tÃ©cnicamente la soluciÃ³n

---

## ğŸ“Š Dataset

**Fuente:** NYC Taxi Trips (viajes de taxi con informaciÃ³n de tarifas y coordenadas)

| Propiedad | Valor |
|-----------|-------|
| **Formato** | CSV |
| **TamaÃ±o** | > 100 MB |
| **UbicaciÃ³n Raw** | `s3://ulacit-datos-masivos/input/dataset.csv` |
| **UbicaciÃ³n Clean** | `s3://ulacit-datos-masivos/output/dataset_limpio.csv` |

### Esquema de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Campo               â”‚ DescripciÃ³n                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ key                 â”‚ Identificador Ãºnico del registro       â”‚
â”‚ fare_amount         â”‚ Monto de la tarifa (USD)               â”‚
â”‚ pickup_datetime     â”‚ Fecha y hora de inicio del viaje       â”‚
â”‚ pickup_longitude    â”‚ Longitud del punto de recogida         â”‚
â”‚ pickup_latitude     â”‚ Latitud del punto de recogida          â”‚
â”‚ dropoff_longitude   â”‚ Longitud del destino                   â”‚
â”‚ dropoff_latitude    â”‚ Latitud del destino                    â”‚
â”‚ passenger_count     â”‚ NÃºmero de pasajeros                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Transformaciones Aplicadas

```mermaid
flowchart LR
    A[CSV Raw] --> B{ValidaciÃ³n}
    B -->|VÃ¡lido| C[NormalizaciÃ³n]
    B -->|InvÃ¡lido| D[Descarte]
    C --> E[CSV Limpio]
    
    style A fill:#ff9900,color:#000
    style E fill:#00a86b,color:#fff
    style D fill:#dc3545,color:#fff
```

- âœ… EliminaciÃ³n de filas con valores vacÃ­os o invÃ¡lidos
- âœ… NormalizaciÃ³n de `pickup_datetime` â†’ `yyyy-MM-dd HH:mm:ss`
- âœ… ValidaciÃ³n de tipos numÃ©ricos (coordenadas, tarifas, pasajeros)
- âœ… GeneraciÃ³n de CSV optimizado para Athena

---

## ğŸ— Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AWS BIG DATA ARCHITECTURE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚    â”‚          â”‚     â”‚          â”‚     â”‚          â”‚     â”‚          â”‚         â”‚
â”‚    â”‚    S3    â”‚â”€â”€â”€â”€â–¶â”‚  Lambda  â”‚â”€â”€â”€â”€â–¶â”‚    S3    â”‚â”€â”€â”€â”€â–¶â”‚  Athena  â”‚         â”‚
â”‚    â”‚  (Raw)   â”‚     â”‚ (Clean)  â”‚     â”‚ (Clean)  â”‚     â”‚  (SQL)   â”‚         â”‚
â”‚    â”‚          â”‚     â”‚          â”‚     â”‚          â”‚     â”‚          â”‚         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                                                   â”‚               â”‚
â”‚         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   Glue   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                        â”‚ Catalog  â”‚                         â”‚               â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚               â”‚
â”‚                                                             â–¼               â”‚
â”‚                                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                                                      â”‚QuickSightâ”‚          â”‚
â”‚                                                      â”‚(Dashbrd) â”‚          â”‚
â”‚                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos

```mermaid
sequenceDiagram
    participant U as Usuario
    participant S3R as S3 (Raw)
    participant L as Lambda
    participant S3C as S3 (Clean)
    participant A as Athena
    participant Q as QuickSight

    U->>S3R: 1. Upload dataset.csv
    S3R->>L: 2. Trigger processing
    L->>L: 3. Validate & Clean
    L->>S3C: 4. Write clean data
    A->>S3C: 5. Query data (SQL)
    Q->>A: 6. Connect & visualize
    Q->>U: 7. Interactive dashboards
```

---

## â˜ï¸ Servicios AWS Utilizados

| Servicio | Rol | JustificaciÃ³n |
|:--------:|-----|---------------|
| <img src="https://img.shields.io/badge/S3-569A31?style=flat-square&logo=amazon-s3&logoColor=white" /> | Almacenamiento | Alta durabilidad (99.999999999%), escalabilidad ilimitada |
| <img src="https://img.shields.io/badge/Lambda-FF9900?style=flat-square&logo=aws-lambda&logoColor=white" /> | Procesamiento | Serverless, pago por uso, escalado automÃ¡tico |
| <img src="https://img.shields.io/badge/Athena-232F3E?style=flat-square&logo=amazon-aws&logoColor=white" /> | Consultas SQL | Data warehouse serverless, sin administraciÃ³n |
| <img src="https://img.shields.io/badge/Glue-232F3E?style=flat-square&logo=amazon-aws&logoColor=white" /> | CatÃ¡logo | Esquemas centralizados, descubrimiento de datos |
| <img src="https://img.shields.io/badge/QuickSight-232F3E?style=flat-square&logo=amazon-aws&logoColor=white" /> | VisualizaciÃ³n | BI gestionado, integraciÃ³n nativa |

---

## ğŸ“‹ Requisitos Previos

> [!IMPORTANT]
> AsegÃºrate de cumplir con todos los requisitos antes de comenzar.

- [ ] Cuenta de AWS activa con acceso al Free Tier
- [ ] QuickSight habilitado en la misma regiÃ³n que Athena (`us-east-2`)
- [ ] Python 3.12 (para desarrollo local de Lambda)
- [ ] Permisos IAM para S3, Lambda, Athena, Glue y QuickSight

---

## âš¡ Quick Start

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/aws-bigdata-architecture.git
cd aws-bigdata-architecture
```

### 2ï¸âƒ£ Configurar S3

```bash
# Crear bucket
aws s3 mb s3://ulacit-datos-masivos --region us-east-2

# Subir dataset
aws s3 cp dataset.csv s3://ulacit-datos-masivos/input/dataset.csv
```

### 3ï¸âƒ£ Desplegar Lambda

```bash
# Empaquetar funciÃ³n
cd aws/lambda
zip lambda_function.zip lambda_function.py

# Crear funciÃ³n (reemplazar ROLE_ARN)
aws lambda create-function \
    --function-name data-cleaner \
    --runtime python3.12 \
    --handler lambda_function.lambda_handler \
    --role arn:aws:iam::ACCOUNT_ID:role/lambda-s3-role \
    --zip-file fileb://lambda_function.zip
```

### 4ï¸âƒ£ Ejecutar pipeline

```bash
# Invocar Lambda
aws lambda invoke \
    --function-name data-cleaner \
    --payload '{}' \
    response.json

cat response.json
```

---

## ğŸ“– ConfiguraciÃ³n Detallada

### Amazon S3

<details>
<summary><b>ğŸ“ Estructura del Bucket</b></summary>

```
ulacit-datos-masivos/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ dataset.csv          # Dataset original
â”œâ”€â”€ output/
â”‚   â””â”€â”€ dataset_limpio.csv   # Dataset procesado
â””â”€â”€ athena-results/          # Resultados de consultas
```

</details>

### AWS Lambda

<details>
<summary><b>âš™ï¸ ConfiguraciÃ³n de la FunciÃ³n</b></summary>

| ParÃ¡metro | Valor |
|-----------|-------|
| Runtime | Python 3.12 |
| Handler | `lambda_function.lambda_handler` |
| Memory | 256 MB |
| Timeout | 5 minutos |

**Variables de Entorno:**

```bash
BUCKET_NAME=ulacit-datos-masivos
INPUT_KEY=input/dataset.csv
OUTPUT_KEY=output/dataset_limpio.csv
```

</details>

<details>
<summary><b>ğŸ“ CÃ³digo Fuente</b></summary>

```python
import boto3
import csv
import io
from datetime import datetime
import os

s3 = boto3.client('s3')

BUCKET_NAME = os.environ.get("BUCKET_NAME", "ulacit-datos-masivos")
INPUT_KEY   = os.environ.get("INPUT_KEY", "input/dataset.csv")
OUTPUT_KEY  = os.environ.get("OUTPUT_KEY", "output/dataset_limpio.csv")


def es_fila_valida(row):
    """
    Valida y normaliza una fila.
    Returns: (bool, dict | None) - (es_vÃ¡lida, fila_normalizada)
    """
    new_row = dict(row)

    try:
        # Validar fecha/hora
        fecha_raw = (new_row.get("pickup_datetime") or "").strip()
        if not fecha_raw:
            return False, None

        fecha_raw = fecha_raw.replace(" UTC", "")
        dt = datetime.strptime(fecha_raw, "%Y-%m-%d %H:%M:%S")
        new_row["pickup_datetime"] = dt.strftime("%Y-%m-%d %H:%M:%S")

        # Validar fare_amount
        fare_raw = (new_row.get("fare_amount") or "").strip()
        if not fare_raw:
            return False, None
        new_row["fare_amount"] = str(float(fare_raw))

        # Validar passenger_count
        pass_raw = (new_row.get("passenger_count") or "").strip()
        if not pass_raw:
            return False, None
        new_row["passenger_count"] = str(int(float(pass_raw)))

        # Validar coordenadas
        coords = ["pickup_longitude", "pickup_latitude", 
                  "dropoff_longitude", "dropoff_latitude"]
        for col in coords:
            val_raw = (new_row.get(col) or "").strip()
            if not val_raw:
                return False, None
            new_row[col] = str(float(val_raw))

        return True, new_row

    except Exception:
        return False, None


def lambda_handler(event, context):
    """Main handler para procesamiento de datos."""
    
    # Leer archivo de entrada
    obj = s3.get_object(Bucket=BUCKET_NAME, Key=INPUT_KEY)
    data = obj["Body"].read().decode("utf-8").splitlines()

    reader = csv.DictReader(data)
    fieldnames = reader.fieldnames

    # Preparar salida
    output_buffer = io.StringIO()
    writer = csv.DictWriter(output_buffer, fieldnames=fieldnames)
    writer.writeheader()

    filas_totales = 0
    filas_validas = 0

    # Procesar filas
    for row in reader:
        filas_totales += 1
        es_valida, fila_limpia = es_fila_valida(row)
        if es_valida and fila_limpia:
            writer.writerow(fila_limpia)
            filas_validas += 1

    # Escribir resultado
    s3.put_object(
        Bucket=BUCKET_NAME,
        Key=OUTPUT_KEY,
        Body=output_buffer.getvalue()
    )

    return {
        "status": "OK",
        "bucket": BUCKET_NAME,
        "input_key": INPUT_KEY,
        "output_key": OUTPUT_KEY,
        "filas_totales": filas_totales,
        "filas_validas": filas_validas,
        "filas_descartadas": filas_totales - filas_validas
    }
```

</details>

### AWS Athena

<details>
<summary><b>ğŸ—„ï¸ Crear Base de Datos y Tabla</b></summary>

**Crear Database:**

```sql
CREATE DATABASE bigdata_aziel;
```

**Crear Tabla Externa:**

```sql
CREATE EXTERNAL TABLE bigdata_aziel.taxis (
    key STRING,
    fare_amount DOUBLE,
    pickup_datetime TIMESTAMP,
    pickup_longitude DOUBLE,
    pickup_latitude DOUBLE,
    dropoff_longitude DOUBLE,
    dropoff_latitude DOUBLE,
    passenger_count INT
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
    "separatorChar" = ",",
    "quoteChar" = "\"",
    "escapeChar" = "\\"
)
LOCATION 's3://ulacit-datos-masivos/output/'
TBLPROPERTIES (
    "skip.header.line.count"="1",
    "use.null.for.invalid.data"="true"
);
```

</details>

---

## ğŸ” Consultas de Ejemplo

### Vista RÃ¡pida de Datos

```sql
SELECT *
FROM bigdata_aziel.taxis
LIMIT 10;
```

### Tarifa Promedio

```sql
SELECT
    AVG(fare_amount) AS avg_fare
FROM bigdata_aziel.taxis;
```

### Viajes por Cantidad de Pasajeros

```sql
SELECT
    passenger_count,
    COUNT(*) AS total_viajes
FROM bigdata_aziel.taxis
GROUP BY passenger_count
ORDER BY passenger_count;
```

### Viajes por Hora del DÃ­a

```sql
SELECT
    date_trunc('hour', pickup_datetime) AS hora,
    COUNT(*) AS total_viajes
FROM bigdata_aziel.taxis
GROUP BY 1
ORDER BY 1;
```

---

## ğŸ“ˆ Amazon QuickSight

### ConfiguraciÃ³n de Permisos

En **Manage QuickSight â†’ Security & Permissions**, habilitar acceso a:

- [x] Amazon Athena
- [x] AWS Glue Data Catalog  
- [x] Amazon S3 (lectura de `ulacit-datos-masivos`)

### Crear Dataset

1. Ir a **Datasets â†’ New dataset**
2. Seleccionar **Athena**
3. Elegir base de datos `bigdata_aziel`
4. Seleccionar tabla `taxis`
5. Importar datos o usar modo **Direct Query**

### Visualizaciones Sugeridas

| Tipo | DescripciÃ³n | MÃ©tricas |
|------|-------------|----------|
| ğŸ“Š **Barras** | Viajes por pasajeros | `passenger_count` vs `COUNT(*)` |
| ğŸ“ˆ **LÃ­neas** | Viajes por hora | `pickup_datetime` vs `COUNT(*)` |
| ğŸ—ºï¸ **Heatmap** | DistribuciÃ³n geogrÃ¡fica | `pickup_longitude`, `pickup_latitude` |
| ğŸ“¦ **Boxplot** | DistribuciÃ³n de tarifas | `fare_amount` |

---

## ğŸ’° Costos y Free Tier

> [!TIP]
> Optimiza tus costos siguiendo las mejores prÃ¡cticas.

| Servicio | Modelo de Cobro | Free Tier |
|----------|-----------------|-----------|
| **S3** | GB almacenado + solicitudes | 5 GB gratuitos |
| **Lambda** | Invocaciones + tiempo | 1M invocaciones/mes |
| **Athena** | TB de datos escaneados | - |
| **QuickSight** | SuscripciÃ³n mensual | Trial disponible |

### Mejores PrÃ¡cticas

```
âœ… Trabajar con dataset limpio y sin columnas innecesarias
âœ… Evitar consultas pesadas en Athena
âœ… Usar particionamiento para datasets grandes
âœ… Borrar recursos no utilizados al finalizar
```

---

## ğŸ”’ Seguridad y Privacidad

| Aspecto | ImplementaciÃ³n |
|---------|----------------|
| **IAM** | Roles con privilegios mÃ­nimos (Least Privilege) |
| **S3** | Bucket privado, sin acceso pÃºblico |
| **QuickSight** | Acceso restringido a cuenta autorizada |
| **Datos** | Dataset pÃºblico, sin informaciÃ³n sensible |

> [!CAUTION]
> Nunca expongas credenciales AWS en el cÃ³digo. Usa variables de entorno o AWS Secrets Manager.

---

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ ğŸ“„ README.md                     # Esta documentaciÃ³n
â”œâ”€â”€ ğŸ“ aws/
â”‚   â””â”€â”€ ğŸ“ lambda/
â”‚       â””â”€â”€ ğŸ“„ lambda_function.py    # CÃ³digo de la funciÃ³n Lambda
â”œâ”€â”€ ğŸ“ sql/
â”‚   â”œâ”€â”€ ğŸ“„ db.sql       # Script creaciÃ³n de DB
â””â”€â”€ ğŸ“ docs/
    â”œâ”€â”€ ğŸ–¼ï¸ arquitectura.png          # Diagrama de arquitectura
    â””â”€â”€ ğŸ“„ informe.pdf               # Informe acadÃ©mico
```

---

## ğŸ—ºï¸ Roadmap

- [ ] **Particionamiento** - Particionar datos en S3 por fecha para mejor rendimiento
- [ ] **ETL Avanzado** - Integrar AWS Glue Jobs para flujos complejos
- [ ] **Machine Learning** - Incorporar modelos predictivos sobre dataset limpio
- [ ] **AutomatizaciÃ³n** - Pipeline CI/CD para actualizaciÃ³n automÃ¡tica de dashboards
- [ ] **Monitoring** - CloudWatch dashboards para mÃ©tricas operativas

---

## ğŸ‘¤ Autor

<div align="center">

**Aziel Quesada**

*IngenierÃ­a InformÃ¡tica - ULACIT*

[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/azielq)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/azielquesada)
[![Website](https://img.shields.io/badge/Website-4285F4?style=for-the-badge&logo=google-chrome&logoColor=white)](https://azielquesada.dev)

</div>

---

<div align="center">

### â­ Â¿Te fue Ãºtil este proyecto?

Si este proyecto te ayudÃ³, considera darle una estrella en GitHub.

---

**Made with â¤ï¸ in Costa Rica ğŸ‡¨ğŸ‡·**

*Proyecto acadÃ©mico para ULACIT - 2025*

</div>
